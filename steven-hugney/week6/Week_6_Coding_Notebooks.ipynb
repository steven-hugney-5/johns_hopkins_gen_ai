{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQLFotRZPMaG"
      },
      "source": [
        "Let’s implement a basic attention mechanism in Python to reinforce the concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QonBRCaJPMaH",
        "outputId": "a95af3cd-638c-4ff1-b026-b814d5d427ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention Scores: [2 0 0]\n",
            "Attention Weights: [0.78698604 0.10650698 0.10650698]\n",
            "Attention Output: [3.61540927]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define query, keys, and values as vectors\n",
        "query = np.array([1, 0, 1])  # Represents \"love\"\n",
        "keys = np.array([[1, 0, 1],  # Represents \"pizza\"\n",
        "                 [0, 1, 0],  # Represents \"but\"\n",
        "                 [1, 0, -1]])  # Represents \"olives\"\n",
        "values = np.array([[5], [0], [-3]])  # Sentiment scores for \"pizza,\" \"but,\" \"olives\"\n",
        "\n",
        "# Calculate attention scores (dot product of query and keys)\n",
        "scores = np.dot(keys, query)\n",
        "\n",
        "# Apply softmax to get weights\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "weights = softmax(scores)\n",
        "\n",
        "# Weighted sum of values\n",
        "attention_output = np.dot(weights, values)\n",
        "\n",
        "print(\"Attention Scores:\", scores)\n",
        "print(\"Attention Weights:\", weights)\n",
        "print(\"Attention Output:\", attention_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy4GBnqgPMaJ"
      },
      "source": [
        "To improve stability for high-dimensional vectors, we scale the dot product by dividing by the square root of the key dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aokQ07hPMaJ",
        "outputId": "876a0a7f-b6e6-4b37-8b21-cbb159f83905"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scaled Attention Scores: [1.15470054 0.         0.        ]\n",
            "Attention Weights: [0.61338261 0.19330869 0.19330869]\n",
            "Attention Output: [2.48698697]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define query, keys, values, and scaling factor\n",
        "query = np.array([1, 0, 1])\n",
        "keys = np.array([[1, 0, 1],\n",
        "                 [0, 1, 0],\n",
        "                 [1, 0, -1]])\n",
        "values = np.array([[5], [0], [-3]])\n",
        "scale = np.sqrt(query.shape[0])  # d_k = dimensionality of the query\n",
        "\n",
        "# Calculate scaled attention scores\n",
        "scores = np.dot(keys, query) / scale\n",
        "\n",
        "# Apply softmax to get weights\n",
        "weights = softmax(scores)\n",
        "\n",
        "# Weighted sum of values\n",
        "attention_output = np.dot(weights, values)\n",
        "\n",
        "print(\"Scaled Attention Scores:\", scores)\n",
        "print(\"Attention Weights:\", weights)\n",
        "print(\"Attention Output:\", attention_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b04tlJcPMaJ"
      },
      "source": [
        "## Self Attention\n",
        "Let’s implement a simple self-attention mechanism for a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHtUf4V3PMaJ",
        "outputId": "c57a7333-a2de-48c2-dbb6-a711c68e6df9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention Weights:\n",
            "[[0.13996839 0.23453985 0.24576871 0.15175171 0.22797133]\n",
            " [0.01847211 0.30437435 0.39016773 0.02861227 0.25837354]\n",
            " [0.01474143 0.30522766 0.39994043 0.02367127 0.25641921]\n",
            " [0.10588597 0.2546468  0.27557477 0.12146871 0.24242375]\n",
            " [0.02037275 0.30283715 0.38578421 0.03107175 0.25993415]]\n",
            "Attention Output:\n",
            "[[1.04688219 1.35331457 0.98510099]\n",
            " [1.30097982 1.68605596 1.22570457]\n",
            " [1.31055465 1.69852508 1.23472785]\n",
            " [1.11290351 1.43982809 1.04766729]\n",
            " [1.29621874 1.67979842 1.22119359]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define structured sentence embeddings (each row represents a word)\n",
        "# The embeddings represent semantic roles:\n",
        "# \"The\" (determiner), \"cat\" (subject noun), \"sat\" (verb), \"on\" (preposition), \"mat\" (object noun)\n",
        "sentence_embeddings = np.array([\n",
        "    [0.1, 0.1, 0.2],  # \"The\"  (low influence determiner)\n",
        "    [0.9, 0.8, 0.7],  # \"cat\"  (subject noun, strong influence)\n",
        "    [0.8, 0.9, 0.8],  # \"sat\"  (verb, central word, strong influence)\n",
        "    [0.2, 0.2, 0.3],  # \"on\"   (preposition, weak influence but linked to \"mat\")\n",
        "    [0.7, 0.6, 0.9]   # \"mat\"  (object noun, linked to \"on\" and \"sat\", but not \"the\")\n",
        "])\n",
        "\n",
        "# Define structured weight matrices for Query (Q), Key (K), and Value (V)\n",
        "# These weights are manually structured to enhance word relationships\n",
        "\n",
        "# Query weight matrix (W_q) (3x3)\n",
        "# Controls how much influence each word has when \"asking for context\"\n",
        "W_q = np.array([\n",
        "    [0.6, 0.2, 0.1],  # Slight attention to structure words\n",
        "    [0.8, 0.7, 0.6],  # \"cat\" has high query influence\n",
        "    [0.7, 0.9, 0.8]   # \"sat\" is the central querying word\n",
        "])\n",
        "\n",
        "# Key weight matrix (W_k) (3x3)\n",
        "# Controls how words \"store\" information for queries to access\n",
        "W_k = np.array([\n",
        "    [0.6, 0.3, 0.2],  # \"The\" contributes weakly to keys\n",
        "    [0.8, 0.7, 0.5],  # \"cat\" stores important information\n",
        "    [0.7, 0.9, 0.6]   # \"sat\" stores strong reference points\n",
        "])\n",
        "\n",
        "# Value weight matrix (W_v) (3x3)\n",
        "# Controls how much information each word contributes to the final representation\n",
        "W_v = np.array([\n",
        "    [0.2, 0.5, 0.3],  # \"The\" contributes little meaning\n",
        "    [0.7, 0.8, 0.6],  # \"Cat\" contributes strongly\n",
        "    [0.8, 0.9, 0.7]   # \"Sat\" contributes highly\n",
        "])\n",
        "\n",
        "# Compute Queries (Q), Keys (K), and Values (V) for each word\n",
        "Q = sentence_embeddings @ W_q  # Transform embeddings into Queries\n",
        "K = sentence_embeddings @ W_k  # Transform embeddings into Keys\n",
        "V = sentence_embeddings @ W_v  # Transform embeddings into Values\n",
        "\n",
        "# Compute attention scores using scaled dot-product attention\n",
        "scores = Q @ K.T  # Compute raw attention scores (similarity between Q and K)\n",
        "\n",
        "# Scale scores to stabilize training (common practice in attention models)\n",
        "d_k = K.shape[1]  # Dimension of keys\n",
        "scores /= np.sqrt(d_k)  # Scaling factor\n",
        "\n",
        "# Apply softmax to obtain normalized attention weights\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Subtract max value for numerical stability\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)  # Normalize rows\n",
        "\n",
        "weights = softmax(scores)  # Compute final attention weights\n",
        "\n",
        "# Compute weighted sum of values to get attention output\n",
        "attention_output = weights @ V  # Weighted combination of V based on attention\n",
        "\n",
        "# Print the attention matrix (how words attend to each other)\n",
        "print(\"Attention Weights:\")\n",
        "print(weights)\n",
        "\n",
        "# Print the final contextualized word representations\n",
        "print(\"Attention Output:\")\n",
        "print(attention_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwD5IpOnPMaK"
      },
      "source": [
        "## Multi-Head Attention\n",
        "Let’s implement a simple version of multi-head attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pg9DIMLPMaL",
        "outputId": "d2e01a79-0284-40d8-e7ca-e3395434935e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Head 1:\n",
            "Attention Weights:\n",
            "[[0.19687765 0.20256664 0.20184662 0.19757994 0.20112916]\n",
            " [0.1727621  0.2232503  0.21620917 0.17838832 0.20939011]\n",
            " [0.17568776 0.22065491 0.21445791 0.18076445 0.20843496]\n",
            " [0.193778   0.20513864 0.20368291 0.19516293 0.20223752]\n",
            " [0.17863977 0.21806136 0.21269322 0.18314844 0.20745723]]\n",
            "Attention Output:\n",
            "[[0.39804471]\n",
            " [0.41973964]\n",
            " [0.41707689]\n",
            " [0.40080164]\n",
            " [0.41439898]]\n",
            "\n",
            "\n",
            "Head 2:\n",
            "Attention Weights:\n",
            "[[0.19921608 0.20052285 0.20071023 0.19940224 0.20014861]\n",
            " [0.19376598 0.20417066 0.20570196 0.19521925 0.20114215]\n",
            " [0.19299284 0.20468993 0.20641784 0.19462201 0.20127738]\n",
            " [0.19843347 0.20104528 0.20142119 0.1988045  0.20029556]\n",
            " [0.19531638 0.20313068 0.20427224 0.19641403 0.20086666]]\n",
            "Attention Output:\n",
            "[[0.08126514]\n",
            " [0.08229866]\n",
            " [0.08244593]\n",
            " [0.08141305]\n",
            " [0.08200382]]\n",
            "\n",
            "\n",
            "Final Multi-Head Attention Output:\n",
            "[[0.39804471 0.08126514]\n",
            " [0.41973964 0.08229866]\n",
            " [0.41707689 0.08244593]\n",
            " [0.40080164 0.08141305]\n",
            " [0.41439898 0.08200382]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define sentence embeddings for each word in \"The cat sat on the mat\"\n",
        "# Each word is represented as a 3-dimensional vector (for simplicity)\n",
        "sentence_embeddings = np.array([\n",
        "    [0.1, 0.1, 0.2],  # \"The\"  (low influence)\n",
        "    [0.9, 0.8, 0.7],  # \"cat\"  (high influence)\n",
        "    [0.8, 0.9, 0.8],  # \"sat\"  (central word)\n",
        "    [0.2, 0.2, 0.3],  # \"on\"   (context word)\n",
        "    [0.7, 0.6, 0.9]   # \"mat\"  (linked to \"sat\" and \"on\")\n",
        "])\n",
        "\n",
        "# Multi-head attention will have two heads for this example\n",
        "num_heads = 2\n",
        "head_dim = sentence_embeddings.shape[1] // num_heads  # Dimension per head\n",
        "\n",
        "# Function to create weight matrices for each head\n",
        "# We'll create separate W_q, W_k, W_v for each head\n",
        "np.random.seed(42)  # Seed for reproducibility\n",
        "def generate_weights(num_heads, head_dim):\n",
        "    return [\n",
        "        (np.random.rand(head_dim, head_dim),  # W_q for this head\n",
        "         np.random.rand(head_dim, head_dim),  # W_k for this head\n",
        "         np.random.rand(head_dim, head_dim))  # W_v for this head\n",
        "        for _ in range(num_heads)\n",
        "    ]\n",
        "\n",
        "# Generate separate weight matrices for each head\n",
        "multi_head_weights = generate_weights(num_heads, head_dim)\n",
        "\n",
        "# Define softmax function for numerical stability\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Perform multi-head attention\n",
        "all_attention_outputs = []\n",
        "for head_index, (W_q, W_k, W_v) in enumerate(multi_head_weights):\n",
        "    # Slice the sentence embeddings for this head\n",
        "    # Each head will process a different projection of the embeddings\n",
        "    head_embeddings = sentence_embeddings[:, head_index * head_dim : (head_index + 1) * head_dim]\n",
        "\n",
        "    # Compute Queries (Q), Keys (K), and Values (V) for this head\n",
        "    Q = head_embeddings @ W_q  # Transform embeddings into Queries\n",
        "    K = head_embeddings @ W_k  # Transform embeddings into Keys\n",
        "    V = head_embeddings @ W_v  # Transform embeddings into Values\n",
        "\n",
        "    # Compute attention scores using scaled dot-product attention\n",
        "    scores = Q @ K.T  # Compute raw attention scores (similarity between Q and K)\n",
        "    d_k = K.shape[1]  # Dimension of keys\n",
        "    scores /= np.sqrt(d_k)  # Scale scores for numerical stability\n",
        "\n",
        "    # Apply softmax to obtain normalized attention weights\n",
        "    attention_weights = softmax(scores)\n",
        "\n",
        "    # Compute weighted sum of values to get the attention output for this head\n",
        "    attention_output = attention_weights @ V\n",
        "\n",
        "    # Store the attention output for this head\n",
        "    all_attention_outputs.append(attention_output)\n",
        "\n",
        "    # Print detailed information for this head\n",
        "    print(f\"Head {head_index + 1}:\")\n",
        "    print(\"Attention Weights:\")\n",
        "    print(attention_weights)\n",
        "    print(\"Attention Output:\")\n",
        "    print(attention_output)\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Concatenate the outputs from all heads to form the final multi-head attention output\n",
        "final_attention_output = np.concatenate(all_attention_outputs, axis=1)\n",
        "\n",
        "# Print the final multi-head attention output\n",
        "print(\"Final Multi-Head Attention Output:\")\n",
        "print(final_attention_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eOFhX1zPMaL"
      },
      "source": [
        "## Encoder-Only Models\n",
        "Let's introduce the Bidirectional encoder representations from transformers (BERT) to create sentence embeddings.  The BERT model represents words with a 768-dimensional vector that captures the contextual meaning of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e92bb75990f14336989d1267286b47a3"
          ]
        },
        "id": "VXEYv9DcPMaM",
        "outputId": "cee4bc4d-23cd-49d8-83a9-89c768015df6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e92bb75990f14336989d1267286b47a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:  17%|#6        | 73.4M/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Input IDs: tensor([[  101,  1996,  4937,  2938,  2006,  1996, 13523,  1012,   102]])\n",
            "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "Sentence Embedding Shape: torch.Size([1, 768])\n",
            "Sentence Embedding Vector: tensor([[-1.8180e-01, -2.6618e-01, -2.1887e-01,  2.1089e-01,  2.8473e-01,\n",
            "         -1.7185e-01, -1.6588e-01,  5.0974e-01, -1.2715e-01, -1.6971e-01,\n",
            "          3.0352e-02, -4.6905e-01, -3.5898e-02,  1.3398e-01, -1.1766e-01,\n",
            "         -2.4077e-01,  1.2072e-01,  5.9153e-02, -3.9102e-01,  1.0781e-01,\n",
            "          2.3168e-01, -2.0653e-01, -5.2181e-01,  9.9232e-02,  2.9413e-01,\n",
            "         -2.4380e-01,  7.1086e-02, -1.4326e-01, -5.0724e-02, -2.2996e-02,\n",
            "          2.1027e-01, -5.6707e-02, -1.4975e-01, -2.7953e-01,  5.4398e-02,\n",
            "         -8.8523e-02,  2.9781e-01,  3.1730e-01, -5.4683e-01,  2.3623e-01,\n",
            "         -3.6286e-01, -1.8020e-01,  2.5657e-02,  5.8190e-01,  4.0848e-01,\n",
            "         -2.3206e-01,  4.0671e-01, -2.0782e-01,  6.3488e-01,  1.6949e-01,\n",
            "         -6.2998e-01,  3.3577e-01, -2.8222e-02,  1.0912e-01, -3.3113e-02,\n",
            "          6.6779e-01,  2.2382e-01, -3.4467e-01, -1.0597e-02,  3.8314e-01,\n",
            "         -6.3685e-02,  4.3446e-01,  1.9478e-01, -5.2195e-01, -2.3798e-01,\n",
            "          4.4900e-01, -6.2229e-02,  2.0469e-01, -3.8803e-01,  2.6863e-01,\n",
            "         -4.2530e-01, -3.8221e-01, -1.1183e-01,  3.4520e-02,  3.5491e-02,\n",
            "          3.3936e-02,  1.3373e-02,  3.4056e-01,  3.3538e-01,  7.5321e-03,\n",
            "         -2.4909e-01,  7.3309e-01, -5.0392e-02,  4.0413e-01,  1.5924e-01,\n",
            "         -9.6006e-02, -3.5544e-01, -3.2521e-01, -4.9352e-01,  3.9558e-01,\n",
            "         -4.1028e-01, -2.2501e-02, -3.1151e-01,  5.4961e-01, -3.4197e-03,\n",
            "          3.4299e-01,  1.4100e-01, -1.2401e-01,  1.8118e-01,  3.1187e-01,\n",
            "          2.7959e-01, -4.7075e-02,  3.9381e-01,  5.5641e-01, -1.9630e-01,\n",
            "         -5.3562e-01,  2.2281e-01, -7.3732e-02, -3.0736e-01,  5.4388e-02,\n",
            "         -7.6831e-02, -4.4882e-01,  3.4053e-01, -9.0910e-02, -3.0797e-02,\n",
            "          7.9998e-01, -1.2165e-01,  1.0552e-02, -2.7447e-01,  1.5263e-01,\n",
            "         -6.9459e-02, -3.7851e-01, -2.9470e-02,  6.5596e-01,  1.0721e-02,\n",
            "          9.9605e-02, -2.2720e-01, -1.3291e-02,  1.2819e-01, -5.4876e-01,\n",
            "          3.8863e-01,  6.1657e-01,  1.8163e-01, -7.4544e-01, -1.9307e-01,\n",
            "          6.9152e-01,  4.8539e-01,  2.7766e-02, -8.7722e-01,  1.3045e-01,\n",
            "          2.8806e-01, -1.0507e-01,  2.2219e-01,  1.7836e-01,  7.5351e-01,\n",
            "          2.5732e-01, -8.6775e-02, -3.6772e-01,  6.5946e-02,  2.5172e-01,\n",
            "          2.3804e-01, -5.2963e-02, -1.4164e-01, -2.6934e-01, -3.5502e-01,\n",
            "         -4.6631e-02, -2.1760e-01,  2.8432e-01,  2.7232e-01,  1.2819e-01,\n",
            "          4.1904e-01, -4.2988e-01, -1.3727e-01,  1.2675e-01, -6.3968e-02,\n",
            "          2.6335e-01, -5.5488e-02,  4.2207e-01, -4.1363e-03,  3.3108e-01,\n",
            "         -4.3492e-01,  2.2446e-01,  9.7977e-01, -3.1159e-01, -2.7761e-01,\n",
            "         -1.3163e-01, -1.9154e-01, -2.1462e-01, -3.6309e-01,  1.8386e-01,\n",
            "         -1.4444e+00,  2.0277e-01,  3.5538e-01,  5.0478e-02,  2.4624e-01,\n",
            "         -2.5637e-01,  6.2883e-01, -7.8285e-01, -3.5928e-01,  3.1781e-01,\n",
            "          1.1742e-01, -4.9752e-01, -5.9380e-01,  1.6201e-01,  5.0663e-01,\n",
            "         -6.3019e-01, -5.5565e-01, -7.1649e-01, -2.2751e-01, -2.0529e-01,\n",
            "          1.0508e-01, -5.5261e-01,  3.3865e-01,  3.1490e-01,  2.3746e-01,\n",
            "          2.0914e-01, -1.7940e-01, -2.9223e-01, -2.8091e-01,  4.0636e-01,\n",
            "         -3.7665e-01,  4.4192e-01,  2.3523e-01,  4.7021e-01,  2.8296e-01,\n",
            "         -3.0547e-02,  1.8078e-01,  2.1675e-01, -2.1941e-01,  2.0466e-01,\n",
            "          1.8458e-02,  2.1619e-01, -6.3532e-01,  5.7675e-01, -6.3234e-02,\n",
            "          1.3103e+00, -9.1458e-03, -1.5545e-01, -2.5181e-01,  6.3300e-01,\n",
            "          2.1394e-02, -1.0540e-03,  5.0959e-01,  5.6181e-01, -4.2664e-01,\n",
            "         -2.3251e-01, -4.5527e-01, -2.1833e-01,  5.1528e-02, -1.3741e-01,\n",
            "         -1.2998e-01,  2.5672e-01,  9.5764e-01, -1.8243e-01,  1.4505e-01,\n",
            "          9.2458e-02,  1.7922e-01,  3.9997e-01, -3.5114e-01, -4.9639e-01,\n",
            "          2.4190e-01, -6.4145e-01,  9.3477e-02, -8.8054e-01, -4.0972e-01,\n",
            "         -3.3682e-01, -5.1631e-01,  7.5997e-02,  5.7851e-03,  2.6265e-01,\n",
            "          5.2102e-01, -2.1065e-02,  8.0029e-01, -2.8990e-01, -3.5991e-02,\n",
            "         -8.0459e-02,  5.1703e-01,  1.7001e-01, -3.1078e-01, -3.4854e-01,\n",
            "          3.3556e-01, -3.4285e-01,  3.6209e-01,  1.6169e-02, -4.8038e-01,\n",
            "         -2.8854e-01,  3.7499e-01, -1.5153e-01,  1.4228e-01, -6.9191e-01,\n",
            "          2.2381e-01,  5.0632e-01, -8.1507e-01,  6.1169e-02,  1.5172e-01,\n",
            "         -6.8361e-01, -9.4351e-02, -3.9218e-03, -7.7626e-02, -3.5472e-01,\n",
            "          1.6956e-02,  5.9929e-01, -1.6488e-01, -3.3955e-01, -5.2725e-01,\n",
            "          7.4708e-02,  8.2415e-02,  4.4449e-01,  2.6306e-01, -8.6217e-02,\n",
            "         -6.4435e-02, -1.5523e-01,  5.7723e-02,  3.4809e-01, -7.3449e-02,\n",
            "         -1.6260e-01, -8.1058e-02, -1.0386e-01, -2.8010e+00,  2.0187e-01,\n",
            "          1.9738e-01, -8.1533e-02,  5.9201e-01, -6.2801e-03, -1.9555e-01,\n",
            "         -3.0112e-01, -8.3316e-01,  2.2188e-01,  1.1744e-01, -4.2947e-01,\n",
            "          6.9756e-01,  4.4782e-01,  3.7165e-01, -4.1186e-01,  1.9253e-01,\n",
            "         -3.3823e-01, -1.9667e-01,  2.8810e-01, -4.4228e-01, -6.7798e-01,\n",
            "         -1.9688e-01, -7.5271e-01,  2.8145e-01,  9.5801e-01,  4.2333e-02,\n",
            "          2.1086e-01, -5.8779e-01, -2.9930e-01, -1.3813e-01, -6.7275e-02,\n",
            "          2.9040e-01, -2.3558e-01,  1.5075e-01, -1.0111e-01,  2.6810e-01,\n",
            "         -2.0281e-01,  5.8822e-02,  2.7638e-01, -1.9381e-01,  1.4305e-01,\n",
            "          2.4323e-01,  2.8183e-01,  7.6759e-01,  2.1849e-02, -3.2285e-01,\n",
            "         -6.3984e-01,  5.1559e-02,  5.0712e-01,  1.9714e-01,  2.0228e-02,\n",
            "         -4.1199e-01,  2.3743e-03,  2.0567e-01, -1.7810e-01,  3.6253e-01,\n",
            "         -1.2272e-01, -5.1544e-01, -3.3257e-01,  3.0418e-02, -4.2342e-01,\n",
            "         -5.4119e-02,  5.0214e-01,  1.8367e-01, -2.7864e-01, -4.0269e-01,\n",
            "         -5.2195e-01,  2.9518e-01,  2.6127e-01, -1.1443e-01, -2.1813e-01,\n",
            "         -4.3794e-01, -1.0423e+00, -5.2256e-01,  2.1939e-01,  4.0848e-01,\n",
            "         -4.0668e-01,  9.8806e-02, -4.5942e-01, -1.4165e-01, -7.7179e-01,\n",
            "         -1.5175e-01, -2.5724e-01, -3.5210e-01, -9.1775e-01,  1.5176e-01,\n",
            "         -4.2168e-01, -2.8450e-01, -4.5297e-01, -7.6861e-02,  3.9813e-01,\n",
            "          5.2478e-02,  3.0200e-01,  8.3216e-02, -1.7473e-01,  2.7818e-01,\n",
            "         -5.3748e-01, -1.7460e-01,  1.5870e-02, -1.1746e-01, -4.0099e-01,\n",
            "          1.5436e-01,  4.9337e-01,  4.7556e-03,  1.7125e-01, -6.2344e-01,\n",
            "         -2.2668e-01, -1.8897e-01,  9.4951e-02,  1.5982e-01, -7.9984e-01,\n",
            "          6.7358e-01, -3.7375e-01, -1.3326e-02, -7.9682e-02,  6.0536e-01,\n",
            "          4.1966e-01, -3.8238e-01,  1.2825e-01,  3.4543e-01,  3.3541e-01,\n",
            "          1.1525e-03, -2.6172e-01, -1.5828e-01, -1.6380e-01,  5.3633e-02,\n",
            "         -7.9598e-01, -1.7977e-01, -1.1096e-01,  1.0655e-01, -3.9408e-01,\n",
            "          7.0108e-02,  2.7183e-01,  2.1945e-01, -2.7339e-01, -6.9180e-01,\n",
            "         -8.3987e-01,  3.3651e-01,  1.3104e-01, -4.6212e-01,  4.4273e-01,\n",
            "          8.0083e-02, -5.8213e-02, -2.2767e-01,  2.5426e-01,  2.9738e-01,\n",
            "         -2.8467e-01, -2.3713e-01,  4.6597e-01, -2.8731e-01,  8.3955e-02,\n",
            "          1.8518e-01, -6.3584e-02,  1.0637e-01, -3.8968e-01,  7.3266e-01,\n",
            "         -1.7899e-01,  1.5397e-01, -4.3563e-01, -2.1165e-01,  1.1570e-01,\n",
            "          5.2629e-01,  4.0577e-01, -2.2937e-01,  1.7624e-01,  6.4670e-01,\n",
            "          2.8960e-01, -1.1457e-01,  4.1702e-01, -6.1832e-02, -4.1925e-01,\n",
            "          6.3936e-03,  4.8000e-01, -5.5363e-01,  3.9111e-01, -7.9442e-02,\n",
            "         -4.9003e-01, -5.9950e-01,  1.3111e-01, -6.9069e-02, -1.0919e-01,\n",
            "         -6.4848e-01, -9.7921e-02,  1.9918e-01,  3.7299e-01,  1.5409e-02,\n",
            "         -3.7443e-01,  2.6693e-01,  2.1462e-01,  3.8449e-02,  6.0391e-01,\n",
            "         -1.7011e-01, -1.2340e-01, -1.0755e+00, -2.9202e-01,  1.1520e-01,\n",
            "          2.4288e-01,  3.7333e-01, -2.4075e-01,  2.7568e-01,  2.5996e-01,\n",
            "         -9.9101e-02, -1.0136e-01, -1.7232e-01, -4.7763e-01,  3.1247e-01,\n",
            "         -1.9466e-01, -1.6855e-01,  5.9275e-02, -4.2303e-01, -6.5450e-01,\n",
            "         -5.5392e-01, -6.3400e-01,  3.0351e-01,  4.8076e-01,  3.1405e-01,\n",
            "         -2.2940e-01,  1.7346e-01, -4.4485e-02, -6.5256e-01, -6.6589e-02,\n",
            "         -1.5051e-01, -4.3322e-01, -7.4733e-01, -1.0733e-01,  7.7828e-02,\n",
            "         -1.1268e-01, -2.9509e-01,  1.0487e-01,  1.5823e-01, -1.3865e-01,\n",
            "          1.5776e-01, -9.4106e-02,  5.6690e-01, -2.6907e-02, -3.0334e-01,\n",
            "         -3.2802e-01,  1.6007e-01,  3.2341e-02,  4.8107e-01,  1.7140e-01,\n",
            "          3.3999e-01, -3.7224e-01,  1.0081e-01, -3.2237e-01,  7.0938e-02,\n",
            "          4.3762e-01, -2.2147e-01,  2.0318e-01,  8.9554e-03,  4.1629e-01,\n",
            "         -1.1936e-01, -3.3468e-01, -2.0736e-01,  4.8982e-03, -3.8980e-01,\n",
            "          1.0930e-01,  2.5941e-01,  2.7195e-01,  5.1520e-02, -2.3167e-01,\n",
            "         -1.8102e-01,  3.8293e-01,  2.3807e-01, -1.0787e-01,  8.4143e-02,\n",
            "         -3.1401e-01,  3.6324e-01, -1.4717e-01,  1.6023e-02, -3.0932e-01,\n",
            "          1.2239e-01, -2.3950e-01, -3.1152e-01,  1.5949e-01,  4.0237e-01,\n",
            "         -9.7055e-01, -6.9757e-02, -3.8795e-01,  7.1729e-02,  6.0893e-01,\n",
            "         -1.8324e-01,  1.7957e-01, -5.4900e-01, -7.9425e-02, -3.8853e-01,\n",
            "          2.2397e-01, -3.7232e-01, -1.4456e-01,  2.7117e-01,  9.1982e-02,\n",
            "          3.4789e-01,  3.7932e-01,  1.7648e-02,  2.6442e-01,  4.7837e-01,\n",
            "          2.5697e-01, -1.2676e-01,  2.0253e-01, -4.4516e-01,  4.7409e-01,\n",
            "          3.3851e-01,  1.4821e-01, -7.0850e-02,  1.0429e-01,  7.3686e-02,\n",
            "         -2.0868e-01, -1.5762e-02,  2.5240e-01, -1.1055e-01, -2.3645e-01,\n",
            "          4.6166e-01,  4.8103e-01, -7.4401e-01,  2.4899e-01, -3.9597e-01,\n",
            "         -2.7084e-01, -2.3521e-01,  3.1707e-01,  2.2322e-01, -2.4710e-01,\n",
            "          2.5126e-01, -2.3708e-02,  2.3607e-01,  6.9342e-01, -5.6823e-01,\n",
            "          1.8407e-01,  2.9944e-01, -1.6406e-01, -3.0632e-01,  4.1097e-01,\n",
            "          2.7652e-01, -2.2538e-02,  1.3380e-01, -3.4514e-01,  3.0766e-02,\n",
            "          5.0891e-01,  8.8038e-02, -5.1622e-01, -4.0305e-03,  5.9627e-02,\n",
            "          3.2955e-01,  2.5616e-01,  3.9327e-01, -2.7031e-01,  1.6456e-01,\n",
            "         -5.9425e-01,  6.6523e-01,  1.6039e-01,  2.8778e-01,  3.2813e-01,\n",
            "          3.2880e-02, -1.7606e-01,  3.5365e-01,  4.7375e-01,  1.9747e-01,\n",
            "          7.9976e-02, -1.5170e-01,  4.0311e-01,  5.5339e-01,  7.0478e-01,\n",
            "          1.7651e-01, -3.3746e-01, -6.0561e-02,  7.1096e-01, -7.0139e-02,\n",
            "         -6.6897e-01, -3.3383e-01,  8.8803e-03,  6.0813e-01,  3.1335e-01,\n",
            "          7.3599e-01,  1.0772e-01,  7.6670e-02,  6.0194e-01,  2.1698e-01,\n",
            "         -3.7470e-01, -4.8618e-01,  3.4693e-01,  2.3011e-01,  1.6590e-01,\n",
            "         -5.1093e-01, -4.1763e-01,  1.9416e-01, -1.8764e-01,  4.1429e-03,\n",
            "         -2.6534e-03,  1.9557e-01, -2.1351e-01,  1.7440e-02, -2.4724e-01,\n",
            "          6.0631e-01, -4.6106e-01,  2.3903e-02, -2.2756e-01,  5.0013e-01,\n",
            "          1.4496e-01,  2.4658e-01, -1.3996e-01,  4.9501e-01, -4.6521e-01,\n",
            "         -2.8496e-01,  1.1279e-01,  8.4733e-02, -4.6487e-02, -2.2940e-01,\n",
            "         -2.7320e-01, -1.0565e-01, -4.7150e-01, -1.1779e-01,  4.7523e-01,\n",
            "         -7.0713e-01,  4.5822e-02,  1.4060e-01, -6.1576e-02,  6.3198e-02,\n",
            "          4.5912e-02, -6.2184e-01,  5.2689e-01,  1.7236e-01, -3.0194e-02,\n",
            "          6.5080e-02,  3.9355e-01, -5.5543e-01, -5.3011e-01,  6.7696e-01,\n",
            "          5.0494e-01,  6.8723e-02, -1.1764e-01, -3.9860e-01,  3.4340e-01,\n",
            "          5.9922e-02, -1.8813e-01, -1.6446e-01,  1.1582e-02, -4.1464e-02,\n",
            "          4.8620e-02, -2.3613e-01, -4.7705e-01,  1.6632e-01, -1.5508e-01,\n",
            "          3.8427e-01,  3.5926e-01, -7.6859e-01, -4.6089e-01, -2.7690e-01,\n",
            "         -4.8478e-01,  3.0774e-01, -6.7386e-01,  1.2245e-01,  3.2314e-01,\n",
            "         -6.0865e-02, -3.8581e-01,  6.7434e-02, -2.3499e-01, -1.7648e-01,\n",
            "         -1.4755e-01,  4.0146e-01, -3.5041e-02]])\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n",
        "\n",
        "# Import required libraries\n",
        "from transformers import BertTokenizer, BertModel  # Pretrained BERT tokenizer and model\n",
        "import torch  # For tensor manipulation (similar to NumPy)\n",
        "\n",
        "# Load a pretrained BERT model and tokenizer\n",
        "# BERT base uncased: lowercase version of the model trained on English text\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Put the model in evaluation mode (not training)\n",
        "model.eval()\n",
        "\n",
        "# Step 1: Define a sentence for embedding\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "\n",
        "# Step 2: Tokenize the sentence\n",
        "# Tokenization converts the sentence into tokens that BERT understands.\n",
        "# BERT uses WordPiece tokenization, which splits words into subwords if necessary.\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")  # \"pt\" indicates PyTorch tensors\n",
        "print(\"Tokenized Input IDs:\", inputs['input_ids'])\n",
        "print(\"Attention Mask:\", inputs['attention_mask'])\n",
        "\n",
        "# Explanation of Tokenized Output:\n",
        "# input_ids: Each word/subword is converted to an integer representing its vocabulary index.\n",
        "# attention_mask: A binary mask indicating which tokens are real (1) and which are padding (0).\n",
        "\n",
        "# Step 3: Pass the tokenized inputs through the BERT model\n",
        "with torch.no_grad():  # Disable gradient calculation (not needed for inference)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Step 4: Extract the hidden states from BERT's output\n",
        "# BERT returns two outputs: the last hidden state and the pooled output.\n",
        "# The last hidden state contains embeddings for each token in the input sentence.\n",
        "last_hidden_state = outputs.last_hidden_state  # Shape: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "# Step 5: Average the token embeddings to create a sentence embedding\n",
        "# This is a simple way to get a fixed-size vector representing the entire sentence.\n",
        "sentence_embedding = last_hidden_state.mean(dim=1)  # Average across the sequence length dimension\n",
        "\n",
        "# Print the sentence embedding\n",
        "print(\"Sentence Embedding Shape:\", sentence_embedding.shape)\n",
        "print(\"Sentence Embedding Vector:\", sentence_embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdNYbHZ0PMaM"
      },
      "source": [
        "## Text Classification with BERT\n",
        "Let’s use the Hugging Face Transformers library to fine-tune BERT for text classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP-5cLltPMaM",
        "outputId": "4ba9a058-f8e1-47b6-bec5-d753bbf68f80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities: tensor([[0.5083, 0.4917],\n",
            "        [0.5016, 0.4984]])\n",
            "Predictions: tensor([0, 0])\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model for classification\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Example input text\n",
        "texts = [\"I love this product!\", \"The movie was terrible.\"]\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# Convert logits to probabilities and predictions\n",
        "probs = torch.softmax(logits, dim=1)\n",
        "predictions = torch.argmax(probs, dim=1)\n",
        "\n",
        "print(\"Probabilities:\", probs)\n",
        "print(\"Predictions:\", predictions)  # 0 for negative, 1 for positive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dy1OexKPMaM"
      },
      "source": [
        "## Text Generation with GPT\n",
        "Let’s generate text using a pre-trained GPT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS5p-lsSPMaM",
        "outputId": "5d91099a-c11b-448e-ed3f-b49074073746"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Once upon a time in a faraway land, a man named Tiberius, who had been a soldier in the army of the Romans, was sent to the city of Rome to be a priest. He was a man of great wealth and great\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained GPT tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"Once upon a time in a faraway land,\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(inputs, max_length=50, do_sample=True, temperature=0.1)\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZknMovKOPMaN"
      },
      "source": [
        "## Language Translation with T5\n",
        "Let’s translate a sentence using the T5 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53rW5o2KPMaN",
        "outputId": "901b28b8-2dea-47b2-bbce-6162b352a268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (0.2.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ianmcculloh/myenvGL/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translation: Le temps est ensoleillé.\n"
          ]
        }
      ],
      "source": [
        "# uinstall necessary libraries\n",
        "!pip install sentencepiece\n",
        "\n",
        "#load libraries\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load pre-trained T5 tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Input text for translation\n",
        "input_text = \"translate English to French: The weather is sunny.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate translation\n",
        "output = model.generate(inputs.input_ids, max_length=50)\n",
        "\n",
        "# Decode the generated translation\n",
        "translation = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Translation:\", translation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ANXTFVEPMaN"
      },
      "source": [
        "Let's use T5 for text summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSWZ08iiPMaN",
        "outputId": "4518a401-b41b-4284-bea0-a72a08098262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary: the painting is considered an archetypal masterpiece of the italian Renaissance. it has been described as the most famous, most visited, most written about, and most sung about work of art.\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the pre-trained T5 tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")  # \"t5-small\" is a smaller, faster version of T5\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Input text for summarization\n",
        "text = \"\"\"\n",
        "The Mona Lisa is a half-length portrait painting by the Italian artist Leonardo da Vinci.\n",
        "It is considered an archetypal masterpiece of the Italian Renaissance, and it has been described\n",
        "as the most famous, most visited, most written about, and most sung about work of art in the world.\n",
        "\"\"\"\n",
        "\n",
        "# Prepare the text for the T5 model\n",
        "# T5 treats all tasks as text-to-text; for summarization, we prepend \"summarize: \" to the input text\n",
        "input_text = \"summarize: \" + text\n",
        "\n",
        "# Tokenize the input text and convert it to a PyTorch tensor\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "# Generate the summary\n",
        "summary_ids = model.generate(\n",
        "    inputs,\n",
        "    max_length=50,          # Maximum length of the summary\n",
        "    num_beams=4,            # Beam search with 4 beams for more coherent output\n",
        "    early_stopping=True     # Stop once an optimal summary is found\n",
        ")\n",
        "\n",
        "# Decode the generated summary back to text\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated summary\n",
        "print(\"Summary:\", summary)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
