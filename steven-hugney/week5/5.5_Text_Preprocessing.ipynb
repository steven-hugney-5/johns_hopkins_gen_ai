{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the spacy package if you have not already done so.  Once you have installed spacy, you no longer need to run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (0.19.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (2.3.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (2.11.9)\n",
      "Requirement already satisfied: jinja2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
      "Requirement already satisfied: wrapt in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from jinja2->spacy) (3.0.3)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the pandas and spacy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data\n",
    "Create some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Twitter data\n",
    "data = {\n",
    "    \"tweet\": [\n",
    "        \"I love the new iPhone! It's fantastic. #Apple\",\n",
    "        \"The service at this restaurant was terrible. Never going back. #Disappointed\",\n",
    "        \"Tesla's new model is groundbreaking! #Innovation\",\n",
    "        \"I had an average experience with the product. It's okay. #Neutral\",\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "The first technical step in preprocessing is tokenization, which breaks down text into smaller units called tokens.  Tokens are the building blocks for all subsequent preprocessing steps. It enables models to process text effectively by isolating meaningful components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0      I love the new iPhone! It's fantastic. #Apple   \n",
      "1  The service at this restaurant was terrible. N...   \n",
      "2   Tesla's new model is groundbreaking! #Innovation   \n",
      "3  I had an average experience with the product. ...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [I, love, the, new, iPhone, !, It, 's, fantast...  \n",
      "1  [The, service, at, this, restaurant, was, terr...  \n",
      "2  [Tesla, 's, new, model, is, groundbreaking, !,...  \n",
      "3  [I, had, an, average, experience, with, the, p...  \n"
     ]
    }
   ],
   "source": [
    "# Tokenization with spaCy\n",
    "df[\"tokens\"] = df[\"tweet\"].apply(lambda x: [token.text for token in nlp(x)])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words\n",
    "Next, we clean the text by removing stop words.  Stop words are common words like “the,” “is,” “and,” which occur frequently but provide little semantic value.  Removing them reduces noise and allows the model to focus on meaningful terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0      I love the new iPhone! It's fantastic. #Apple   \n",
      "1  The service at this restaurant was terrible. N...   \n",
      "2   Tesla's new model is groundbreaking! #Innovation   \n",
      "3  I had an average experience with the product. ...   \n",
      "\n",
      "                                              tokens  \n",
      "0     [love, new, iPhone, !, fantastic, ., #, Apple]  \n",
      "1  [service, restaurant, terrible, ., going, ., #...  \n",
      "2  [Tesla, new, model, groundbreaking, !, #, Inno...  \n",
      "3  [average, experience, product, ., okay, ., #, ...  \n"
     ]
    }
   ],
   "source": [
    "# Function to remove stop words\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop]  # Exclude stop words\n",
    "    return tokens\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df[\"tokens\"] = df[\"tweet\"].apply(remove_stopwords)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "After cleaning, we standardize text using stemming or lemmatization. Stemming reduces words to their root form by removing suffixes and prefixes. Lemmatization converts words to their base or dictionary form using linguistic rules.  Lemmitization is more accurate than stemming but computationally heavier. Note the clock difference between running the stop word code block and the lemmitization code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0      I love the new iPhone! It's fantastic. #Apple   \n",
      "1  The service at this restaurant was terrible. N...   \n",
      "2   Tesla's new model is groundbreaking! #Innovation   \n",
      "3  I had an average experience with the product. ...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0     [love, new, iPhone, !, fantastic, ., #, Apple]   \n",
      "1  [service, restaurant, terrible, ., going, ., #...   \n",
      "2  [Tesla, new, model, groundbreaking, !, #, Inno...   \n",
      "3  [average, experience, product, ., okay, ., #, ...   \n",
      "\n",
      "                                      tokens_no_stop  \\\n",
      "0     [love, new, iPhone, !, fantastic, ., #, Apple]   \n",
      "1  [service, restaurant, terrible, ., going, ., #...   \n",
      "2  [Tesla, new, model, groundbreaking, !, #, Inno...   \n",
      "3  [average, experience, product, ., okay, ., #, ...   \n",
      "\n",
      "                                              lemmas  \n",
      "0     [love, new, iPhone, !, fantastic, ., #, Apple]  \n",
      "1  [service, restaurant, terrible, ., go, ., #, d...  \n",
      "2  [tesla, new, model, groundbreaking, !, #, inno...  \n",
      "3  [average, experience, product, ., okay, ., #, ...  \n"
     ]
    }
   ],
   "source": [
    "# Function to perform lemmatization on stopword-removed tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    doc = nlp(\" \".join(tokens))  # Recreate a string from tokens\n",
    "    lemmas = [token.lemma_ for token in doc]  # Lemmatize the tokens\n",
    "    return lemmas\n",
    "\n",
    "# Apply stop word removal\n",
    "df[\"tokens_no_stop\"] = df[\"tweet\"].apply(remove_stopwords)\n",
    "\n",
    "# Apply lemmatization on tokens without stop words\n",
    "df[\"lemmas\"] = df[\"tokens_no_stop\"].apply(lemmatize_tokens)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Visualize the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Original Tweet  \\\n",
      "0      I love the new iPhone! It's fantastic. #Apple   \n",
      "1  The service at this restaurant was terrible. N...   \n",
      "2   Tesla's new model is groundbreaking! #Innovation   \n",
      "3  I had an average experience with the product. ...   \n",
      "\n",
      "                                              Tokens  \\\n",
      "0       love, new, iPhone, !, fantastic, ., #, Apple   \n",
      "1  service, restaurant, terrible, ., going, ., #,...   \n",
      "2  Tesla, new, model, groundbreaking, !, #, Innov...   \n",
      "3  average, experience, product, ., okay, ., #, N...   \n",
      "\n",
      "                                   Lemmatized Tokens  \n",
      "0       love, new, iPhone, !, fantastic, ., #, Apple  \n",
      "1  service, restaurant, terrible, ., go, ., #, di...  \n",
      "2  tesla, new, model, groundbreaking, !, #, innov...  \n",
      "3  average, experience, product, ., okay, ., #, N...  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Tweet</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Lemmatized Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love the new iPhone! It's fantastic. #Apple</td>\n",
       "      <td>love, new, iPhone, !, fantastic, ., #, Apple</td>\n",
       "      <td>love, new, iPhone, !, fantastic, ., #, Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The service at this restaurant was terrible. N...</td>\n",
       "      <td>service, restaurant, terrible, ., going, ., #,...</td>\n",
       "      <td>service, restaurant, terrible, ., go, ., #, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tesla's new model is groundbreaking! #Innovation</td>\n",
       "      <td>Tesla, new, model, groundbreaking, !, #, Innov...</td>\n",
       "      <td>tesla, new, model, groundbreaking, !, #, innov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had an average experience with the product. ...</td>\n",
       "      <td>average, experience, product, ., okay, ., #, N...</td>\n",
       "      <td>average, experience, product, ., okay, ., #, N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Original Tweet  \\\n",
       "0      I love the new iPhone! It's fantastic. #Apple   \n",
       "1  The service at this restaurant was terrible. N...   \n",
       "2   Tesla's new model is groundbreaking! #Innovation   \n",
       "3  I had an average experience with the product. ...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0       love, new, iPhone, !, fantastic, ., #, Apple   \n",
       "1  service, restaurant, terrible, ., going, ., #,...   \n",
       "2  Tesla, new, model, groundbreaking, !, #, Innov...   \n",
       "3  average, experience, product, ., okay, ., #, N...   \n",
       "\n",
       "                                   Lemmatized Tokens  \n",
       "0       love, new, iPhone, !, fantastic, ., #, Apple  \n",
       "1  service, restaurant, terrible, ., go, ., #, di...  \n",
       "2  tesla, new, model, groundbreaking, !, #, innov...  \n",
       "3  average, experience, product, ., okay, ., #, N...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new DataFrame for visualization\n",
    "df_visualization = pd.DataFrame({\n",
    "    \"Original Tweet\": df[\"tweet\"],\n",
    "    \"Tokens\": df[\"tokens_no_stop\"].apply(lambda x: \", \".join(x)),  # Display tokens without stop words\n",
    "    \"Lemmatized Tokens\": df[\"lemmas\"].apply(lambda x: \", \".join(x))  # Display lemmatized tokens\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_visualization)\n",
    "\n",
    "# Optional: Render the DataFrame as an HTML table for better visualization (e.g., in a Jupyter Notebook)\n",
    "from IPython.display import display\n",
    "display(df_visualization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
