{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing Code Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries for this demonstration. Note that you may need to install libraries if you have not used them before.  If you have already loaded these libraries skip the pip install step and move straight to importing the necessary libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk>=3.9->textblob)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk>=3.9->textblob)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>=3.9->textblob)\n",
      "  Downloading regex-2025.9.18-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk>=3.9->textblob)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2025.9.18-cp312-cp312-macosx_11_0_arm64.whl (287 kB)\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk, textblob\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [textblob]4/6\u001b[0m [nltk]b]\n",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.3.0 joblib-1.5.2 nltk-3.9.1 regex-2025.9.18 textblob-0.19.0 tqdm-4.67.1\n",
      "Collecting googletrans==4.0.0-rc1\n",
      "  Using cached googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Using cached httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting certifi (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting sniffio (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Using cached hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Using cached httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Using cached httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "Using cached h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Using cached h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "Using cached hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Using cached hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Using cached rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17455 sha256=2a814d2603114346abb50ea416092ae75024820e3ba2d783206370dda94e0b8b\n",
      "  Stored in directory: /Users/shugney20/Library/Caches/pip/wheels/95/0f/04/b17a72024b56a60e499ce1a6313d283ed5ba332407155bee03\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, sniffio, idna, hstspreload, h2, certifi, httpcore, httpx, googletrans\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [googletrans]\u001b[0m [idna]\n",
      "\u001b[1A\u001b[2KSuccessfully installed certifi-2025.8.3 chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0 sniffio-1.3.1\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.3.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from transformers) (2025.9.18)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl.metadata (36 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests->transformers) (2.10)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Using cached transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading numpy-2.3.3-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl (205 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, safetensors, pyyaml, numpy, hf-xet, fsspec, filelock, charset_normalizer, requests, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed charset_normalizer-3.4.3 filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.1 numpy-2.3.3 pyyaml-6.0.3 requests-2.32.5 safetensors-0.6.2 tokenizers-0.22.1 transformers-4.56.2 typing-extensions-4.15.0 urllib3-2.5.0\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp312-cp312-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (2.3.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (2.32.5)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting jinja2 (from spacy)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting setuptools (from spacy)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached smart_open-7.3.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Downloading markupsafe-3.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Downloading spacy-3.8.7-cp312-cp312-macosx_11_0_arm64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-macosx_11_0_arm64.whl (42 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp312-cp312-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading preshed-3.0.10-cp312-cp312-macosx_11_0_arm64.whl (126 kB)\n",
      "Using cached pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-macosx_11_0_arm64.whl (634 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.7/634.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.6-cp312-cp312-macosx_11_0_arm64.whl (839 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m839.4/839.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blis-1.3.0-cp312-cp312-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached cloudpathlib-0.22.0-py3-none-any.whl (61 kB)\n",
      "Using cached smart_open-7.3.1-py3-none-any.whl (61 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Downloading marisa_trie-1.3.1-cp312-cp312-macosx_11_0_arm64.whl (155 kB)\n",
      "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading wrapt-1.17.3-cp312-cp312-macosx_11_0_arm64.whl (39 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, typing-inspection, spacy-loggers, spacy-legacy, shellingham, setuptools, pydantic-core, murmurhash, mdurl, MarkupSafe, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, jinja2, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31/31\u001b[0m [spacy]m30/31\u001b[0m [spacy]]e-data]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.22.0 confection-0.1.5 cymem-2.0.11 jinja2-3.1.6 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 markdown-it-py-4.0.0 mdurl-0.1.2 murmurhash-1.0.13 preshed-3.0.10 pydantic-2.11.9 pydantic-core-2.33.2 rich-14.1.0 setuptools-80.9.0 shellingham-1.5.4 smart-open-7.3.1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.19.2 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.3\n",
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp312-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading torch-2.8.0-cp312-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [torch]32m3/4\u001b[0m [torch]kx]\n",
      "\u001b[1A\u001b[2KSuccessfully installed mpmath-1.3.0 networkx-3.5 sympy-1.14.0 torch-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!pip install googletrans==4.0.0-rc1  # Latest compatible version\n",
    "!pip install transformers\n",
    "!pip install spacy\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from textblob import TextBlob\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "from googletrans import Translator\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a toy dataset for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "data = {\n",
    "    \"tweet\": [\n",
    "        \"I love the new iPhone! It's fantastic. #Apple\",\n",
    "        \"The service at this restaurant was terrible. Never going back. #Disappointed\",\n",
    "        \"Tesla's new model is groundbreaking! #Innovation\",\n",
    "        \"I had an average experience with the product. It's okay. #Neutral\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert dataset to DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Classification:\n",
      "                                               tweet  classification\n",
      "0      I love the new iPhone! It's fantastic. #Apple               1\n",
      "1  The service at this restaurant was terrible. N...               0\n",
      "2   Tesla's new model is groundbreaking! #Innovation               1\n",
      "3  I had an average experience with the product. ...               1\n"
     ]
    }
   ],
   "source": [
    "# 1. Text Classification (Binary sentiment classification: positive/negative)\n",
    "def text_classification():\n",
    "    # Label tweets manually for demonstration\n",
    "    df[\"label\"] = [1, 0, 1, 1]  # 1 = positive, 0 = negative\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df[\"tweet\"])\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, df[\"label\"])\n",
    "    predictions = model.predict(X)\n",
    "    df[\"classification\"] = predictions\n",
    "    print(\"\\nText Classification:\")\n",
    "    print(df[[\"tweet\", \"classification\"]])\n",
    "\n",
    "text_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment Analysis (Polarity Scores):\n",
      "                                               tweet  sentiment\n",
      "0      I love the new iPhone! It's fantastic. #Apple   0.356818\n",
      "1  The service at this restaurant was terrible. N...  -0.583333\n",
      "2   Tesla's new model is groundbreaking! #Innovation   0.170455\n",
      "3  I had an average experience with the product. ...   0.175000\n"
     ]
    }
   ],
   "source": [
    "# 2. Sentiment Analysis\n",
    "def sentiment_analysis():\n",
    "    df[\"sentiment\"] = df[\"tweet\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    print(\"\\nSentiment Analysis (Polarity Scores):\")\n",
    "    print(df[[\"tweet\", \"sentiment\"]])\n",
    "\n",
    "sentiment_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entity Recognition (NER):\n",
      "                                               tweet             entities\n",
      "0      I love the new iPhone! It's fantastic. #Apple    [(Apple, PERSON)]\n",
      "1  The service at this restaurant was terrible. N...                   []\n",
      "2   Tesla's new model is groundbreaking! #Innovation       [(Tesla, ORG)]\n",
      "3  I had an average experience with the product. ...  [(Neutral, PERSON)]\n"
     ]
    }
   ],
   "source": [
    "# 3. Named Entity Recognition (NER)\n",
    "def named_entity_recognition():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    df[\"entities\"] = df[\"tweet\"].apply(lambda x: [(ent.text, ent.label_) for ent in nlp(x).ents])\n",
    "    print(\"\\nNamed Entity Recognition (NER):\")\n",
    "    print(df[[\"tweet\", \"entities\"]])\n",
    "\n",
    "named_entity_recognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of Speech (POS) tagging function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parts of Speech (POS) Tagging:\n",
      "                                               tweet  \\\n",
      "0      I love the new iPhone! It's fantastic. #Apple   \n",
      "1  The service at this restaurant was terrible. N...   \n",
      "2   Tesla's new model is groundbreaking! #Innovation   \n",
      "3  I had an average experience with the product. ...   \n",
      "\n",
      "                                            pos_tags  \n",
      "0  [(I, PRON), (love, VERB), (the, DET), (new, AD...  \n",
      "1  [(The, DET), (service, NOUN), (at, ADP), (this...  \n",
      "2  [(Tesla, PROPN), ('s, PART), (new, ADJ), (mode...  \n",
      "3  [(I, PRON), (had, VERB), (an, DET), (average, ...  \n"
     ]
    }
   ],
   "source": [
    "# 4. Parts of Speech (POS) Tagging\n",
    "def pos_tagging():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    df[\"pos_tags\"] = df[\"tweet\"].apply(lambda x: [(token.text, token.pos_) for token in nlp(x)])\n",
    "    print(\"\\nParts of Speech (POS) Tagging:\")\n",
    "    print(df[[\"tweet\", \"pos_tags\"]])\n",
    "\n",
    "pos_tagging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine translation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Machine Translation (English to Spanish):\n",
      "                                               tweet  \\\n",
      "0      I love the new iPhone! It's fantastic. #Apple   \n",
      "1  The service at this restaurant was terrible. N...   \n",
      "2   Tesla's new model is groundbreaking! #Innovation   \n",
      "3  I had an average experience with the product. ...   \n",
      "\n",
      "                                          translated  \n",
      "0        ¡Amo el nuevo iPhone!Es fantástico.#Manzana  \n",
      "1  El servicio en este restaurante fue terrible.N...  \n",
      "2  ¡El nuevo modelo de Tesla es innovador!#Innova...  \n",
      "3  Tuve una experiencia promedio con el producto....  \n"
     ]
    }
   ],
   "source": [
    "# 5. Machine Translation\n",
    "def machine_translation():\n",
    "    translator = Translator()\n",
    "    df[\"translated\"] = df[\"tweet\"].apply(lambda x: translator.translate(x, src=\"en\", dest=\"es\").text)\n",
    "    print(\"\\nMachine Translation (English to Spanish):\")\n",
    "    print(df[[\"tweet\", \"translated\"]])\n",
    "\n",
    "machine_translation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
