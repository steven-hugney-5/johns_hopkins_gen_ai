{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyS2Ovli5gq2"
      },
      "source": [
        "# Prompt Engineering\n",
        "\n",
        "In this notebook, we will demonstrate the fundementals of using LangChain for prompt engineering. Specifically, we will do the following:\n",
        "\n",
        "- create a prompt from a template\n",
        "- create a LLM\n",
        "- create a chain\n",
        "- look at some specialized chains for few-shot prompting and for Chain-Of-Thought\n",
        "\n",
        "For this  exercise, we are going to focus on a classification task. Namely, the classification of the \"stance\" of a comment towards another comment. The base comment is given below:\n",
        "\n",
        "```python\n",
        "comment = \"The new Dune movie does not really capture the vision laid out by Frank Herbert. It feels like they tried to import too many visual effects that take away from the philosophy of the work.\"\n",
        "```\n",
        "\n",
        "The replies to the comment that we will classify for their stance toward the comment as \"agree\", \"disagree\", and \"neutral\" are:\n",
        "\n",
        "```python\n",
        "replies = [\n",
        "    \"The newer ones fail to live up to the sophistry of the older movies from the 70's.\",\n",
        "    \"Frank Herbert wrote a lot of books.\",\n",
        "    \"I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\",\n",
        "    \"The quick red fox jumped over the lazy brown dog.\",\n",
        "    \"Yeah, this new movie is a real masterpiece, lol!!\"\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--I4xWYW5gq7"
      },
      "source": [
        "## Configure the environment\n",
        "\n",
        "I need your help with classifying the stance of replies to comments about a topic using LangChain and Langchain-Huggingface for running local models. First, I need the code to install the neccesary packages from a notebook envrionment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RowDP8ff5gq8",
        "outputId": "5acd83cf-93c8-47a9-e414-bb9d19245bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
            "  Downloading langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting langsmith>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.4.34-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain) (2.11.9)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading sqlalchemy-2.0.44-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain) (6.0.3)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading orjson-3.11.3-cp312-cp312-macosx_15_0_arm64.whl.metadata (41 kB)\n",
            "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading zstandard-0.25.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
            "Collecting anyio (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
            "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: certifi in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (2025.8.3)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: idna in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (2.10)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.79-py3-none-any.whl (449 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
            "Downloading langsmith-0.4.34-py3-none-any.whl (386 kB)\n",
            "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading sqlalchemy-2.0.44-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.11.3-cp312-cp312-macosx_15_0_arm64.whl (127 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Downloading zstandard-0.25.0-cp312-cp312-macosx_11_0_arm64.whl (640 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m640.4/640.4 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
            "Installing collected packages: zstandard, tenacity, SQLAlchemy, orjson, jsonpointer, h11, anyio, requests-toolbelt, jsonpatch, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "\u001b[2K  Attempting uninstall: h11\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [SQLAlchemy]\n",
            "\u001b[2K    Found existing installation: h11 0.9.0━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [SQLAlchemy]\n",
            "\u001b[2K    Uninstalling h11-0.9.0:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [SQLAlchemy]\n",
            "\u001b[2K      Successfully uninstalled h11-0.9.0━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [SQLAlchemy]\n",
            "\u001b[2K  Attempting uninstall: httpcorem╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/15\u001b[0m [anyio]]\n",
            "\u001b[2K    Found existing installation: httpcore 0.9.1━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/15\u001b[0m [anyio]\n",
            "\u001b[2K    Uninstalling httpcore-0.9.1:0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/15\u001b[0m [anyio]\n",
            "\u001b[2K      Successfully uninstalled httpcore-0.9.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/15\u001b[0m [anyio]\n",
            "\u001b[2K  Attempting uninstall: httpx╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/15\u001b[0m [anyio]\n",
            "\u001b[2K    Found existing installation: httpx 0.13.3━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/15\u001b[0m [anyio]\n",
            "\u001b[2K    Uninstalling httpx-0.13.3:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/15\u001b[0m [anyio]\n",
            "\u001b[2K      Successfully uninstalled httpx-0.13.3━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/15\u001b[0m [anyio]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [langchain]15\u001b[0m [langchain]core]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "googletrans 4.0.0rc1 requires httpx==0.13.3, but you have httpx 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SQLAlchemy-2.0.44 anyio-4.11.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.27 langchain-core-0.3.79 langchain-text-splitters-0.3.11 langsmith-0.4.34 orjson-3.11.3 requests-toolbelt-1.0.0 tenacity-9.1.2 zstandard-0.25.0\n",
            "Requirement already satisfied: transformers in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (4.56.2)\n",
            "Requirement already satisfied: filelock in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from transformers) (0.35.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from transformers) (2025.9.18)\n",
            "Requirement already satisfied: requests in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain-huggingface) (0.3.79)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain-huggingface) (0.35.1)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.34)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.25.0)\n",
            "Requirement already satisfied: anyio in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.11.0)\n",
            "Requirement already satisfied: certifi in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.10)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\n",
            "Requirement already satisfied: filelock in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.9.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/shugney20/Documents/genai/.venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.3.1)\n",
            "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: langchain-huggingface\n",
            "Successfully installed langchain-huggingface-0.3.1\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain\n",
        "! pip install transformers\n",
        "! pip install langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3hJLyfM5gq-"
      },
      "source": [
        "## Create a prompt object\n",
        "\n",
        "I need your help with classifying the stance of comments using LangChain. First, I need you to give me the code to create a prompt object, called \"stance_prompt\" from LangChain around the following template:\n",
        "'''Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words after outputing the label.\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "stance:'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NU4fDbQB5gq_",
        "outputId": "dac58bc9-1e60-4d91-9866-ca7c1e850eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words, only the label.\n",
            "comment: I think this policy is not effective.\n",
            "reply: I agree, it doesn't address the core issues.\n",
            "stance:\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define the template for stance classification\n",
        "template = '''Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words, only the label.\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "stance:'''\n",
        "\n",
        "# Create the prompt object\n",
        "stance_prompt = PromptTemplate(\n",
        "    input_variables=[\"comment\", \"reply\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "# Example usage of the prompt\n",
        "comment = \"I think this policy is not effective.\"\n",
        "reply = \"I agree, it doesn't address the core issues.\"\n",
        "prompt = stance_prompt.format(comment=comment, reply=reply)\n",
        "print(prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSSsDeKQ5gq_"
      },
      "source": [
        "## Create an LLM object\n",
        "\n",
        "### Option 1: Using a small encoder-decoder model\n",
        "Now, I need you to create an LLM object using LangChain. In particular, I would like to use the text2text-generation model of \"declare-lab/flan-alpaca-gpt4-xl\" from HuggingFace and use the CPU. Make sure to import the langchain HuggingFace pipeline as \"from langchain_huggingface import HuggingFacePipeline\". Also, make sure when creating the pipeline to specify \"max_new_tokens = 500\", and make sure the pipeline only outputs the generated text and not text from the prompt.\n",
        "\n",
        "```python\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the model using Hugging Face pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"declare-lab/flan-alpaca-gpt4-xl\",\n",
        "    device=0,  # Use GPU (-1 for CPU)\n",
        "    max_new_tokens = 500,\n",
        ")\n",
        "\n",
        "# Create the LangChain LLM using the HuggingFace pipeline\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "# Example usage with the prompt object from before\n",
        "prompt = '''Please classify the stance, or opinion, of the following reply to the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words.\n",
        "comment: I think the new policy will help improve efficiency.\n",
        "reply: I disagree, the policy doesn't address the real issues.\n",
        "stance:'''\n",
        "\n",
        "# Get the model's response\n",
        "response = llm(prompt)\n",
        "print(response)\n",
        "```\n",
        "\n",
        "### Option 2: Using a small decoder-only model\n",
        "Now, I need you to create an LLM object using LangChain. In particular, I would like to use the text-generation model of \"tiiuae/Falcon3-1B-Instruct\" from HuggingFace and use the CPU. Make sure to import the langchain HuggingFace pipeline as \"from langchain_huggingface import HuggingFacePipeline\". Also, make sure when creating the pipeline to specify \"max_new_tokens = 500\", and make sure the pipeline only outputs the generated text and not the prompt.\n",
        "\n",
        "```python\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"tiiuae/Falcon3-1B-Instruct\",\n",
        "    device=0,  # Use GPU (-1 for CPU)\n",
        "    max_new_tokens = 500,\n",
        "    return_full_text=False\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "57503e437a0947068985d92302bb5a99",
            "090f0fb4b5b043df91280f4a05f2a110",
            "4dd918f565a446fa9b8e77b87a9a56fa",
            "f6ed95119f5242ae900c56c0b14dafbc",
            "af778a7c76aa4f8ea1c190394667d8e9",
            "9db64bbd996c4b1eb55346c051848225",
            "ac1dec2fcbd848578e9c87ca57040b30",
            "ed0ff2bb755f41ef9070e8ee0af0b7b3",
            "832e195a263048909254c5ccd87e8c62",
            "2079f966cb1348f29ce87ab007e79eaa"
          ]
        },
        "id": "GcJxbbQG5gq_",
        "outputId": "9d6c565e-a370-400f-a67b-aedb54172d89"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57503e437a0947068985d92302bb5a99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/658 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "z:\\Users\\Iain\\Anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Iain\\.cache\\huggingface\\hub\\models--tiiuae--Falcon3-3B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "090f0fb4b5b043df91280f4a05f2a110",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4dd918f565a446fa9b8e77b87a9a56fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6ed95119f5242ae900c56c0b14dafbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af778a7c76aa4f8ea1c190394667d8e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.47G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9db64bbd996c4b1eb55346c051848225",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac1dec2fcbd848578e9c87ca57040b30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/113 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed0ff2bb755f41ef9070e8ee0af0b7b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/364k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "832e195a263048909254c5ccd87e8c62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.78M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2079f966cb1348f29ce87ab007e79eaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Iain\\AppData\\Local\\Temp\\ipykernel_4388\\2609232209.py:23: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm(prompt)\n",
            "z:\\Users\\Iain\\Anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " disagree\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the model using Hugging Face pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"tiiuae/Falcon3-3B-Instruct\",\n",
        "    device=0,  # Use GPU (-1 for CPU)\n",
        "    max_new_tokens = 500,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# Create the LangChain LLM using the HuggingFace pipeline\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "# Example usage with the prompt object from before\n",
        "prompt = '''Please classify the stance, or opinion, of the following reply to the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words.\n",
        "comment: I think the new policy will help improve efficiency.\n",
        "reply: I disagree, the policy doesn't address the real issues.\n",
        "stance:'''\n",
        "\n",
        "# Get the model's response\n",
        "response = llm(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h0G4UPb5grA"
      },
      "source": [
        "## Create a Chain\n",
        "\n",
        "Now, I would like the python code to create a LangChain Chain from the prompt template \"stance_prompt\" and the LLM \"llm\". Make sure to use the \"|\" syntax for defining the chain. and call the chain by the \"invoke\" method. Please name the chain \"stance_chain\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W18QHavF5grB",
        "outputId": "3e140af9-82f9-444c-f8ac-cbd99222c1c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " agree\n"
          ]
        }
      ],
      "source": [
        "stance_chain = stance_prompt | llm\n",
        "\n",
        "# Example usage: run the chain with the provided comment and reply\n",
        "comment = \"I think the government should invest more in public health.\"\n",
        "reply = \"I agree that public health should be a priority.\"\n",
        "\n",
        "# Format the input and get the result\n",
        "result = stance_chain.invoke({\"comment\": comment, \"reply\": reply})\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgrinvs_5grC"
      },
      "source": [
        "Great. Now, I would like to code to run the previously defined \"stance_chain\" on a comment called \"test_comment\" across each entry in a list called \"test_replies\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9tSvtUB5grC"
      },
      "outputs": [],
      "source": [
        "test_comment = \"The new Dune movie does not really capture the vision laid out by Frank Herbert. It feels like they tried to import too many visual effects that take away from the philosophy of the work.\"\n",
        "\n",
        "test_replies = [\n",
        "    \"The newer ones fail to live up to the sophistry of the older movies from the 70's.\",\n",
        "    \"Frank Herbert wrote a lot of books.\",\n",
        "    \"I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\",\n",
        "    \"The quick red fox jumped over the lazy brown dog.\",\n",
        "    \"Yeah, this new movie is a real masterpiece, lol!!\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHwYueMu5grC",
        "outputId": "3237fb68-38de-4b1f-f089-33c5b5f73598"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply 1: The newer ones fail to live up to the sophistry of the older movies from the 70's.\n",
            "Stance:  agree\n",
            "\n",
            "Reply 2: Frank Herbert wrote a lot of books.\n",
            "Stance:  neutral\n",
            "\n",
            "Reply 3: I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\n",
            "Stance:  disagree\n",
            "\n",
            "Reply 4: The quick red fox jumped over the lazy brown dog.\n",
            "Stance:  neutral\n",
            "\n",
            "Reply 5: Yeah, this new movie is a real masterpiece, lol!!\n",
            "Stance:  disagree\n",
            "\n"
          ]
        }
      ],
      "source": [
        "responses = []\n",
        "for reply in test_replies:\n",
        "    response = stance_chain.invoke({\"comment\": test_comment, \"reply\": reply})\n",
        "    responses.append(response)\n",
        "\n",
        "# Print the results\n",
        "for idx, (reply, response) in enumerate(zip(test_replies, responses)):\n",
        "    print(f\"Reply {idx+1}: {reply}\\nStance: {response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljgw93Vr5grD"
      },
      "source": [
        "# Few-shot Prompt\n",
        "\n",
        "Now, Please create a LangChain FewShotPromptTemplate for classifying the stance of a reply to a comment. Use the following template for each example and create an example prompt using example_prompt for the examples in the few-shot prompt template:\n",
        "\n",
        "comment: [comment]\n",
        "reply: [reply]\n",
        "stance: [stance]\n",
        "\n",
        "Then, use the following structure for the few-shot prompt:\n",
        "prefix = '''Stance classification is the task of determining the expressed or implied opinion, or stance, of a reply toward a comment. The following replies express opinions about the associated comment. Each reply can either be \"agree\", \"disagree\", or \"neutral\" toward the comment.'''\n",
        "\n",
        "suffix = '''Analyze the following reply to the provided comment and determine its stance. Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "stance:'''\n",
        "\n",
        "Create five few-shot examples with different comments and replies, including at least one for each possible stance: \"agree\", \"disagree\", and \"neutral\". Provide the code that constructs the FewShotPromptTemplate using the examples and the given prefix and suffix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLuNW7HJ5grD"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import FewShotPromptTemplate\n",
        "\n",
        "# Define the prompt template for each example\n",
        "example_template = '''comment: {comment}\n",
        "reply: {reply}\n",
        "stance: {stance}'''\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"comment\", \"reply\", \"stance\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# Define the examples with various stances\n",
        "examples = [\n",
        "    {'comment': \"I think the new policy will help improve efficiency.\",\n",
        "     'reply': \"I agree, it will make things more streamlined.\",\n",
        "     'stance': 'agree'},\n",
        "\n",
        "    {'comment': \"The new education reform seems promising.\",\n",
        "     'reply': \"I disagree, it doesn't address the underlying issues.\",\n",
        "     'stance': 'disagree'},\n",
        "\n",
        "    {'comment': \"The park renovation project is a good idea.\",\n",
        "     'reply': \"I’m not sure. It may be good, but the location is an issue.\",\n",
        "     'stance': 'neutral'},\n",
        "\n",
        "    {'comment': \"Artificial intelligence will revolutionize healthcare.\",\n",
        "     'reply': \"I agree, it has the potential to save many lives.\",\n",
        "     'stance': 'agree'},\n",
        "\n",
        "    {'comment': \"The economy is showing signs of recovery after the pandemic.\",\n",
        "     'reply': \"I disagree, the recovery seems to be slow and uneven.\",\n",
        "     'stance': 'disagree'},\n",
        "]\n",
        "\n",
        "# Define the prefix and suffix for the few-shot prompt\n",
        "prefix = '''Stance classification is the task of determining the expressed or implied opinion, or stance, of a reply toward a comment. The following replies express opinions about the associated comment. Each reply can either be \"agree\", \"disagree\", or \"neutral\" toward the comment.'''\n",
        "\n",
        "suffix = '''Analyze the following reply to the provided comment and determine its stance. Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "stance:'''\n",
        "\n",
        "# Create the FewShotPromptTemplate using the provided prefix, suffix, and examples\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"comment\", \"reply\"],\n",
        "    example_separator=\"\\n\"\n",
        ")\n",
        "\n",
        "# Now you can use this prompt in your LangChain pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap4iCn0H5grF",
        "outputId": "a766df8f-3b75-4b62-ec05-1658b3920571"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply 1: The newer ones fail to live up to the sophistry of the older movies from the 70's.\n",
            "Stance:  disagree\n",
            "<|assistant|>\n",
            "disagree\n",
            "\n",
            "Reply 2: Frank Herbert wrote a lot of books.\n",
            "Stance:  disagree\n",
            "<|assistant|>\n",
            "disagree\n",
            "\n",
            "Reply 3: I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\n",
            "Stance:  disagree\n",
            "\n",
            "Reply 4: The quick red fox jumped over the lazy brown dog.\n",
            "Stance:  neutral\n",
            "\n",
            "Reply 5: Yeah, this new movie is a real masterpiece, lol!!\n",
            "Stance:  disagree\n",
            "\n"
          ]
        }
      ],
      "source": [
        "few_shot_chain = few_shot_prompt | llm\n",
        "\n",
        "responses = []\n",
        "for reply in test_replies:\n",
        "    response = few_shot_chain.invoke({\"comment\": test_comment, \"reply\": reply})\n",
        "    responses.append(response)\n",
        "\n",
        "# Print the results\n",
        "for idx, (reply, response) in enumerate(zip(test_replies, responses)):\n",
        "    print(f\"Reply {idx+1}: {reply}\\nStance: {response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIJymZ2o5grF"
      },
      "source": [
        "# Chain-of-Thought Prompt\n",
        "\n",
        "Please generate code that uses chain-of-thought (CoT) prompting to classify the stance of a reply to a comment. The process should consist of two stages:\n",
        "\n",
        "1. First Stage (Explanatory Step):\n",
        "- Generate an explanation for the stance (agree, disagree, neutral) of the reply toward the comment.\n",
        "- Use a prompt template for this step with the variables \"comment\" and \"reply\".\n",
        "- Output: \"stance_reason\".\n",
        "\n",
        "2. Second Stage (Final Classification Step):\n",
        "- Based on the explanation from the first stage (\"stance_reason\"), determine the final stance of the reply.\n",
        "- Use a second prompt template for this step with the variables \"comment\", \"reply\", and \"stance_reason\".\n",
        "- Output: The final stance as \"agree\", \"disagree\", or \"neutral\".\n",
        "\n",
        "Use the \"RunnablePassthrough\" to pass the \"comment\" and \"reply\" variables from the first chain to the second chain, and chain the steps together using the \"|\" operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeyNdhgS5grF"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "cot_template_1 = '''Stance classification is the task of determining the expressed or implied opinion, or stance, of the reply towards the comment.\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "explanation:'''\n",
        "\n",
        "cot_prompt_1 = PromptTemplate(\n",
        "    input_variables=[\"comment\",\"reply\"],\n",
        "    template=cot_template_1\n",
        ")\n",
        "\n",
        "cot_chain_1 = cot_prompt_1 | llm\n",
        "\n",
        "cot_template_2 ='''Therefore, based on your explanation, {stance_reason}, what is the final stance? Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "stance:'''\n",
        "\n",
        "cot_prompt_2 = PromptTemplate(\n",
        "    input_variables=[\"comment\",\"reply\",\"stance_reason\"],\n",
        "    template=cot_template_2\n",
        ")\n",
        "\n",
        "cot_chain_2 = cot_prompt_2 | llm\n",
        "\n",
        "cot_chain = {\"stance_reason\": cot_chain_1, \"comment\":RunnablePassthrough(), \"reply\":RunnablePassthrough()} | cot_chain_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srwt_GQN5grF",
        "outputId": "c0c7765f-44a2-4af5-a7c4-6967775c8500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply 1: The newer ones fail to live up to the sophistry of the older movies from the 70's.\n",
            "Stance:  \"disagree\"\n",
            "\n",
            "Reply 2: Frank Herbert wrote a lot of books.\n",
            "Stance:  neutral\n",
            "\n",
            "Reply 3: I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\n",
            "Stance:  \"disagree\"\n",
            "\n",
            "Reply 4: The quick red fox jumped over the lazy brown dog.\n",
            "Stance: \n",
            "\n",
            "Reply 5: Yeah, this new movie is a real masterpiece, lol!!\n",
            "Stance:  <|assistant|>\n",
            "disagree\n",
            "\n"
          ]
        }
      ],
      "source": [
        "responses = []\n",
        "for reply in test_replies:\n",
        "    response = cot_chain.invoke({\"comment\": test_comment, \"reply\": reply})\n",
        "    responses.append(response)\n",
        "\n",
        "# Print the results\n",
        "for idx, (reply, response) in enumerate(zip(test_replies, responses)):\n",
        "    print(f\"Reply {idx+1}: {reply}\\nStance: {response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7LDGvKW5grG"
      },
      "source": [
        "# Tree-of-Thought Prompt\n",
        "\n",
        "Please generate code that uses Tree-of-Thought (ToT) prompting to explore multiple reasoning paths and iteratively evaluate potential stances (agree, disagree, neutral) before making a final classification. Maintain the context of the comment and reply through all stages of reasoning and evaluation.\n",
        "\n",
        "Steps:\n",
        "Step 1 - Generate Hypotheses:\n",
        "- Propose multiple possible stances (agree, disagree, neutral) based on different interpretations of the reply.\n",
        "- Use a prompt template to generate each hypothesis with explanations.\n",
        "- Output: hypotheses as a list.\n",
        "\n",
        "Step 2 - Evaluate Hypotheses:\n",
        "- Assess the validity of each hypothesis by critically analyzing its reasoning.\n",
        "- Use an evaluation prompt template to rank or score each hypothesis based on coherence and relevance. Assign a score (1–5) for logical consistency and coherence\n",
        "- Output: evaluations as scores or rankings for each hypothesis.\n",
        "\n",
        "Step 3 - Final Decision:\n",
        "- Select the hypothesis with the highest score or best reasoning and output the final stance as \"agree\", \"disagree\", or \"neutral\" based on the reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFnIKEc45grG"
      },
      "outputs": [],
      "source": [
        "# Step 1: Generate Hypotheses\n",
        "hypothesis_template = '''\n",
        "Consider the following comment and reply:\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "\n",
        "Generate three different hypotheses for the stance of the reply towards the comment.\n",
        "For each hypothesis, explain why the reply might:\n",
        "1. Agree\n",
        "2. Disagree\n",
        "3. Be Neutral\n",
        "\n",
        "Output each hypothesis clearly labeled (e.g., \"Hypothesis 1: ...\") with a newline between each hypothesis.'''\n",
        "\n",
        "hypothesis_prompt = PromptTemplate(\n",
        "    input_variables=[\"comment\", \"reply\"],\n",
        "    template=hypothesis_template\n",
        ")\n",
        "\n",
        "hypothesis_chain = hypothesis_prompt | llm\n",
        "\n",
        "# Step 2: Evaluate Hypotheses\n",
        "evaluation_template = '''\n",
        "Consider the following comment and reply:\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "\n",
        "Given the following hypotheses and explanations for the stance of the reply towards the comment:\n",
        "{hypotheses}\n",
        "\n",
        "Evaluate each hypothesis based on its logical consistency and support from the reply.\n",
        "Assign a numerical score from 1 to 5 for each hypothesis, where 5 is highly consistent and 1 is inconsistent. Only reply with the score and reason for that score for each hypothesis (e.g., Hypothesis 1: [score], reason: ...) and no other text.'''\n",
        "\n",
        "evaluation_prompt = PromptTemplate(\n",
        "    input_variables=[\"hypotheses\", \"comment\", \"reply\"],\n",
        "    template=evaluation_template\n",
        ")\n",
        "\n",
        "evaluation_chain = evaluation_prompt | llm\n",
        "\n",
        "# Step 3: Final Decision\n",
        "decision_template = '''\n",
        "Consider the following comment and reply:\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "\n",
        "Based on the evaluations of different hypotheses for the stance of the reply towards the comment:\n",
        "{hypotheses}\n",
        "\n",
        "{evaluations}\n",
        "\n",
        "Select the hypothesis with the highest score. Output the final stance as \"agree\", \"disagree\", or \"neutral\" based on the chosen hypothesis. Only output the label as single word and do not generate any other text after the label.\n",
        "label:'''\n",
        "\n",
        "decision_prompt = PromptTemplate(\n",
        "    input_variables=[\"hypotheses\", \"evaluations\", \"comment\", \"reply\"],\n",
        "    template=decision_template\n",
        ")\n",
        "\n",
        "decision_chain = decision_prompt | llm\n",
        "\n",
        "# Combine the chains into a Tree-of-Thought pipeline\n",
        "tot_chain = {\n",
        "    \"hypotheses\": hypothesis_chain,\n",
        "    \"comment\": RunnablePassthrough(),\n",
        "    \"reply\": RunnablePassthrough()\n",
        "} | RunnablePassthrough.assign(evaluations = evaluation_chain) | decision_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7SpwAAs5grG",
        "outputId": "74902665-b3c8-4fe3-e439-d30098dfac0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply 1: The newer ones fail to live up to the sophistry of the older movies from the 70's.\n",
            "Stance:  agree\n",
            "\n",
            "Reply 2: Frank Herbert wrote a lot of books.\n",
            "Stance:  agree\n",
            "\n",
            "Reply 3: I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\n",
            "Stance:  disagree\n",
            "\n",
            "Reply 4: The quick red fox jumped over the lazy brown dog.\n",
            "Stance:  neutral\n",
            "\n",
            "Reply 5: Yeah, this new movie is a real masterpiece, lol!!\n",
            "Stance:  neutral\n",
            "\n"
          ]
        }
      ],
      "source": [
        "responses = []\n",
        "for reply in test_replies:\n",
        "    response = tot_chain.invoke({\"comment\": test_comment, \"reply\": reply})\n",
        "    responses.append(response)\n",
        "\n",
        "# Print results\n",
        "for idx, (reply, response) in enumerate(zip(test_replies, responses)):\n",
        "    print(f\"Reply {idx+1}: {reply}\\nStance: {response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wfT2bft5grG"
      },
      "source": [
        "# Self-Consistency Prompt\n",
        "\n",
        "Given the final stance labels from multiple reasoning approaches—Tree-of-Thought (ToT), Chain-of-Thought (CoT), Few-Shot prompting, and Task-only approach—determine the final stance label (\"agree\", \"disagree\", or \"neutral\") by synthesizing the outputs.\n",
        "\n",
        "Inputs:\n",
        "- Comment and Reply: The context for determining the stance.\n",
        "- Stance Labels from the four approaches:\n",
        "    - tot_output: Stance label from the Tree-of-Thought (ToT) approach.\n",
        "    - cot_output: Stance label from the Chain-of-Thought (CoT) approach.\n",
        "    - few_shot_output: Stance label from the Few-Shot prompting approach.\n",
        "    - task_output: Stance label from the Task-only approach.\n",
        "\n",
        "Steps:\n",
        "1. Compare the stance labels from the four approaches.\n",
        "2. Identify patterns of agreement or disagreement:\n",
        "- If there is a majority consensus, select that stance label.\n",
        "- If there is no clear majority, resolve the inconsistencies by choosing the most consistent or compelling label based on the distribution of outputs.\n",
        "3. Output: A final stance label of \"agree\", \"disagree\", or \"neutral\", based on the most consistent or majority label. Only output the label, with no additional explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6zgw_ha5grG"
      },
      "outputs": [],
      "source": [
        "# Self-Consistency Prompt Template\n",
        "consistency_template = '''\n",
        "Consider the following comment and reply:\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "\n",
        "You have been provided with stance outputs generated by four different approaches:\n",
        "1. Tree-of-Thought (ToT) approach: {tot_output}\n",
        "2. Chain-of-Thought (CoT) approach: {cot_output}\n",
        "3. Few-Shot approach: {few_shot_output}\n",
        "4. Task-only approach: {task_output}\n",
        "\n",
        "Compare these outputs and determine the most likely stance label. Output the final stance label as \"agree\", \"disagree\", or \"neutral\" based on the most consistency across the responses. Only output the label as single word and do not generate any other text after the label.\n",
        "'''\n",
        "\n",
        "consistency_prompt = PromptTemplate(\n",
        "    input_variables=[\"comment\", \"reply\", \"tot_output\", \"cot_output\", \"few_shot_output\", \"task_output\"],\n",
        "    template=consistency_template\n",
        ")\n",
        "\n",
        "self_evaluation_chain = consistency_prompt | llm\n",
        "\n",
        "# Combine chains into a pipeline with pass-through variables\n",
        "consistency_chain = {\n",
        "    \"tot_output\": tot_chain,\n",
        "    \"cot_output\": cot_chain,\n",
        "    \"few_shot_output\": few_shot_chain,\n",
        "    \"task_output\": stance_chain,\n",
        "    \"comment\": RunnablePassthrough(),\n",
        "    \"reply\": RunnablePassthrough()\n",
        "} | self_evaluation_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkwS4s5-5grG",
        "outputId": "11f4197e-c5b3-4300-ced7-6ffc737a7068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply 1: The newer ones fail to live up to the sophistry of the older movies from the 70's.\n",
            "Stance: disagree\n",
            "\n",
            "Reply 2: Frank Herbert wrote a lot of books.\n",
            "Stance: disagree\n",
            "\n",
            "Reply 3: I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\n",
            "Stance: <|assistant|>\n",
            "disagree\n",
            "\n",
            "Reply 4: The quick red fox jumped over the lazy brown dog.\n",
            "Stance: <|assistant|>\n",
            "neutral\n",
            "\n",
            "Reply 5: Yeah, this new movie is a real masterpiece, lol!!\n",
            "Stance: <|assistant|>\n",
            "disagree\n",
            "\n"
          ]
        }
      ],
      "source": [
        "responses = []\n",
        "for reply in test_replies:\n",
        "    response = consistency_chain.invoke({\"comment\": test_comment, \"reply\": reply})\n",
        "    responses.append(response)\n",
        "\n",
        "# Print results\n",
        "for idx, (reply, response) in enumerate(zip(test_replies, responses)):\n",
        "    print(f\"Reply {idx+1}: {reply}\\nStance: {response}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCu2A_wD5grH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
